{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name: Anshuman Kumar Yadav\n",
    "## Roll No.: 241030018\n",
    "## Kaggle User ID: anshumankumaryadav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries and Functions:\n",
    "These libraries and functions are key for building, training, and evaluating a machine learning model,particularly for classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Defined Function:\n",
    "The function reads data from a CSV-like file, where each line consists of values separated by commas.It splits each line into individual values and appends them to a list.Finally, it converts this list into a pandas DataFrame and returns it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_variable_length_data(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            values = line.strip().split(',')\n",
    "            data.append(values)\n",
    "    return pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Training and Test Data:\n",
    "This code reads two files:\n",
    "1. train.txt: Contains the training data.\n",
    "2. test.txt: Contains the test data.\n",
    "\n",
    "After reading the data it checks if the training or test data is empty (meaning no data was read),if either is empty, an error message is printed, and the program stops using exit(1). This ensures that the program will not continue without valid training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read the training data\n",
    "train_data = read_variable_length_data('train.txt')\n",
    "if train_data.empty:\n",
    "    print(\"Unable to proceed without train data. Please check the file and try again.\")\n",
    "    exit(1)\n",
    "\n",
    "# Read the test data\n",
    "test_data = read_variable_length_data('test.txt')\n",
    "if test_data.empty:\n",
    "    print(\"Unable to proceed without test data. Please check the file and try again.\")\n",
    "    exit(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate Features, Labels for Training Data and prepare Test Data\n",
    "This segment extracts the features and labels from the training data and prepares the features for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Separate features and labels for training data\n",
    "X_train = train_data.iloc[:, 2:].astype(float).values\n",
    "y_train = train_data.iloc[:, 1].values\n",
    "X_test = test_data.iloc[:, 1:].astype(float).values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Defined Function:\n",
    "This function extracts useful statistical features from time series data (like trends or patterns in sequences) and returns an array of the extracted features for each time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Feature Extraction Function\n",
    "def extract_features(time_series):\n",
    "    features = []\n",
    "    for ts in time_series:\n",
    "        ts = ts[~np.isnan(ts)]  # Remove NaN values\n",
    "        if len(ts) == 0:\n",
    "            features.append([0] * 10)  # Add default values for empty time series\n",
    "        else:\n",
    "            features.append([\n",
    "                np.mean(ts),\n",
    "                np.std(ts),\n",
    "                np.max(ts),\n",
    "                np.min(ts),\n",
    "                np.median(ts),\n",
    "                np.percentile(ts, 25),\n",
    "                np.percentile(ts, 75),\n",
    "                np.sum(np.abs(np.diff(ts))),  # Total variation\n",
    "                np.var(ts),  # Variance\n",
    "                np.ptp(ts)  # Peak-to-peak range\n",
    "            ])\n",
    "    return np.array(features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction and Scaling from Training and Test Data:\n",
    "Extraction will convert the raw time series data into a structured set of features that can be fed into a machine learning model.And Scaling scales the feature values to have zero mean and unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract Features\n",
    "X_train_features = extract_features(X_train)\n",
    "X_test_features = extract_features(X_test)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_features)\n",
    "X_test_scaled = scaler.transform(X_test_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the Training Data for Validation:\n",
    "This splits the training data into a new training set and a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split training data for validation\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(X_train_scaled, y_train, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model (Artificial Neural Network - MLPClassifier):\n",
    "This part creates and trains a Multi-Layer Perceptron (MLP) classifier.MLPClassifier is a neural network classifier from sklearn, specifically a multi-layer perceptron (ANN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ANN (MLPClassifier)\n",
    "model = MLPClassifier(\n",
    "    hidden_layer_sizes=(128, 64, 32),\n",
    "    activation='tanh', \n",
    "    solver='lbfgs',  # Works better for small datasets\n",
    "    alpha=0.0001,  # Regularization\n",
    "    max_iter=500,  # Limit iterations to avoid overfitting\n",
    "    early_stopping=True,  \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_split, y_train_split)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation and Prediction:\n",
    "This part evaluates the modelâ€™s performance on the validation data and makes predictions on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Validate the model\n",
    "y_val_pred = model.predict(X_val)\n",
    "val_f1_score = f1_score(y_val, y_val_pred, average='weighted')\n",
    "print(f\"Optimized Validation F1-score: {val_f1_score}\")\n",
    "\n",
    "# Predict on test data\n",
    "y_test_pred = model.predict(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation and Saving of Submission File\n",
    "This creates a submission DataFrame and saves the submission DataFrame to a CSV file (submission.csv).And then this output CSV file is submitted on Kaggle to check scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    'index': test_data.iloc[:, 0],\n",
    "    'label': y_test_pred\n",
    "})\n",
    "\n",
    "# Save submission file\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Optimized submission file 'submission.csv' has been created.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
